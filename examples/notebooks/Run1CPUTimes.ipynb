{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Phosim CPU Times: \n",
    "## What can we learn from Twinkles Run 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twinkles Run 1 is 1227 visits, a subset of the observations of a Deep Drilling Field from the kraken-1042 Opsim simulation.  Each of the visits is simulated as a 30-second observation with Phosim.  Together, the 1227 runs of Phosim required about 4 CPU years on the SLAC batch farm.  The CPU time required per visit varied widely.  Some required only a few hours.  More than 100 visits (representing about one third of the total CPU time) never completed because they reached the 5-day CPU time limit for the batch hosts, and produced no output.  \n",
    "\n",
    "Studying the dependence of the CPU time on the various inputs to the Phosim simulations has several motivations.  Certainly there's no sense in investing CPU time in a run that cannot finish with 5 CPU days.  And Run 1 was relatively modest in terms of resource requirements.  Essentially all 1227 simulations ran in parallel on the batch farm.  For Twinkles1 and 2, which will be much larger, the batch farm will not be able to run all of the simulations in parallel and the effective throughput could well be determined by the individual simulations that take the most CPU time, if they are allowed to hog the available hosts. Twinkles1 and 2 might run at NERSC, which has more hosts, but reportedly a 2-day CPU limit per host.  Absent some solution involving checkpointing, we may need to make judicious adjustments to the Phosim inputs for these runs.  Also, John Peterson has pointed out that we should not assume that all of the inputs to Phosim for Run 1 were sensible.\n",
    "\n",
    "For this study, a number of Phosim inputs were extracted from the instance catalogs for the runs, and combined with execution time information from the batch farm.  The columns of the resulting csv file are described [here](https://github.com/DarkEnergyScienceCollaboration/Twinkles/issues/159).\n",
    "\n",
    "This Notebook is based on Phil Marshall's [machine learning example](https://github.com/drphilmarshall/StatisticalMethods/blob/master/examples/SDSScatalog/Quasars.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For pretty plotting\n",
    "# !pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import copy\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "* This is a regression problem, to be able to predict the CPU time in terms of various metadata related to the runs.  These apparently are called \"features\" in machine learning terminology.  Also the CPU time will be referred to as the \"response variable.\"\n",
    "\n",
    "* Read in the data.  Here just selected columns are being read from the file.  *They may or may not be the most relevant.  Also, the filter and hostname columns would need to be converted to numerical values in order to be used.  And the runlimit column probably needs special treatment because it is clearly related to large CPU times*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run1meta = pd.read_csv(\"http://www.slac.stanford.edu/~digel/lsst/run1_metadata.csv\",index_col=0,usecols=[\"obshistid\",\"expmjd\",\"filter\",\"rotskypos\",\"altitude\",\\\n",
    "                                                \"rawseeing\",\"airmass\",\"sunalt\",\"moonalt\",\"moonphase\",\"dist2moon\",\"cputime\",\\\n",
    "                                                \"runlimit\"])\n",
    "\n",
    "# Omit the two runs that have not actually finished running (or been \n",
    "# terminated at the CPU time limit).  These are flagged with -999.0 in\n",
    "# the input file.  Also omit the runs that reached the CPU limit:\n",
    "run1meta = run1meta[(run1meta[\"cputime\"] > 0.) & (run1meta[\"runlimit\"] == 0)]\n",
    "\n",
    "#run1meta = run1meta[(run1meta[\"cputime\"] > 0.)]\n",
    "\n",
    "# Response variables: cputime\n",
    "cputime = np.log(run1meta[\"cputime\"])\n",
    "\n",
    "# Features or attributes: photometric measurements\n",
    "run1meta_features = copy.copy(run1meta)\n",
    "del run1meta_features[\"cputime\"]\n",
    "del run1meta_features[\"runlimit\"]\n",
    "run1meta_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins =  hist(cputime.values,bins=100) ; xlabel(\"CPU time (s)\") ; ylabel(\"N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of CPU time has a very long tail, plus a number of runs piled up at the 5-day CPU time limit.\n",
    "\n",
    "Let's plot all the features, colored by the CPU time, to look for structure.  (Possibly the logarithm of the CPU time should be used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Truncate the color at 3e5 CPU sec just to keep some contrast.\n",
    "norm = mpl.colors.Normalize(vmin=min(cputime.values), vmax=13)\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# Plot everything against everything else:\n",
    "rez = pd.scatter_matrix(run1meta_features,alpha=0.2,figsize=[15,15],color=m.to_rgba(cputime.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our machine learning inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = run1meta_features.values  # Data: 5-d feature space\n",
    "y = cputime.values # Target: redshifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Design matrix shape =\", X.shape)\n",
    "print(\"Response variable vector shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Let's follow the same procedure as in the [`SciKit-Learn` tutorial](../../scikit-learn/Linear_Regression.ipynb) we just went through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "linear = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the model, using all the attributes:\n",
    "linear.fit(X_train, y_train)\n",
    "\n",
    "# Do the prediction on the test data:\n",
    "y_lr_pred = linear.predict(X_test)\n",
    "\n",
    "# How well did we do?\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_linear = np.sqrt(mean_squared_error(y_test,y_lr_pred))\n",
    "r2_linear = linear.score(X_test, y_test)\n",
    "print(\"Linear regression: MSE = \",mse_linear)\n",
    "print(\"R2 score =\",r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(y_test,y_lr_pred - y_test,'o',alpha=0.2)\n",
    "title(\"Linear Regression Residuals - MSE = %.2f\" % mse_linear)\n",
    "xlabel(\"CPU Time\")\n",
    "ylabel(\"Residual\")\n",
    "ylim(-3,2)\n",
    "hlines(0,min(y_test),max(y_test),color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just how bad is this? Here's the MSE from guessing the *average redshift of the training set* for all new objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Naive MSE\", ((1./len(y_train))*(y_train - y_train.mean())**2).sum())\n",
    "print(\"Linear regression: MSE = \",mse_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_squared_error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *k*-Nearest Neighbor (KNN) Regression\n",
    "\n",
    "Now let's try a different kind of model: a *non-parametric* one.\n",
    "\n",
    "[\"Regression based on k-nearest neighbors. The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.\"](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "\n",
    "#### Question:\n",
    "\n",
    "What underlying model is implied by the KNN algorithm? How many hidden parameters does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled_knn = preprocessing.scale(X) # Many methods work better on scaled X.\n",
    "\n",
    "KNN = neighbors.KNeighborsRegressor(5)\n",
    "\n",
    "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X_scaled_knn, y)\n",
    "\n",
    "KNN.fit(X_train_knn,y_train_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_knn_pred = KNN.predict(X_test_knn)\n",
    "mse_knn = mean_squared_error(y_test_knn,y_knn_pred)\n",
    "r2_knn = KNN.score(X_test_knn, y_test_knn)\n",
    "print(\"MSE (KNN) =\", mse_knn)\n",
    "print(\"R2 score (KNN) =\",r2_knn)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (linear regression) = \",mse_linear)\n",
    "print(\"R2 score (linear regression) =\",r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(y_test_knn, y_knn_pred - y_test_knn,'o',alpha=0.2)\n",
    "#plot(y_test, y_lr_pred - y_test,'x')\n",
    "title(\"k-NN Residuals - MSE = %.2f\" % mse_knn)\n",
    "xlabel(\"CPU Time\")\n",
    "ylabel(\"Residual\")\n",
    "ylim(-3,2)\n",
    "hlines(0,min(y_test),max(y_test),color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tuning the KNN Model\n",
    "\n",
    "* Let's vary the control parameters of the KNN model, to see how good we can make our predictions.\n",
    "\n",
    "* We can see our options in the model `repr`:\n",
    "\n",
    "> KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_neighbors=5, p=2, weights='uniform')\n",
    "\n",
    "* Let's first make a \"validation curve\" to investigate one parameter: the number of nearest neighbors averaged over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll vary the number of neighbors used:\n",
    "param_name = \"n_neighbors\"\n",
    "param_range = np.array([1,2,4,8,16,32,64])\n",
    "\n",
    "# And we'll need a cv iterator:\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "shuffle_split = ShuffleSplit(len(X_scaled_knn), 10, test_size=0.4)\n",
    "\n",
    "# Compute our cv scores for a range of the no. of neighbors:\n",
    "from sklearn.learning_curve import validation_curve\n",
    "training_scores, validation_scores = validation_curve(KNN, X_scaled_knn, y,\n",
    "                                                      param_name=param_name,\n",
    "                                                      param_range=param_range, \n",
    "                                                      cv=shuffle_split, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_validation_curve(param_name,parameter_values, training_scores, validation_scores):\n",
    "    training_scores_mean = np.mean(training_scores, axis=1)\n",
    "    training_scores_std = np.std(training_scores, axis=1)\n",
    "    validation_scores_mean = np.mean(validation_scores, axis=1)\n",
    "    validation_scores_std = np.std(validation_scores, axis=1)\n",
    "\n",
    "    plt.fill_between(parameter_values, training_scores_mean - training_scores_std,\n",
    "                     training_scores_mean + training_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(parameter_values, validation_scores_mean - validation_scores_std,\n",
    "                     validation_scores_mean + validation_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(parameter_values, training_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(parameter_values, validation_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.ylim(validation_scores_mean.min() - .1, training_scores_mean.max() + .1)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_validation_curve(param_name, param_range, training_scores, validation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMMENT HERE!\n",
    "\n",
    "Blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model tuning with `GridSearchCV`\n",
    "\n",
    "* Now, let's see if we can do better by varying some other KNN options as well - in a *grid search*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': np.array([1,2,4,8,16,32,64]),\n",
    "                  'weights': ['uniform','distance'],\n",
    "                       'p' : np.array([1,2])}\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "KNN_tuned = GridSearchCV(KNN, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNN_tuned.fit(X_train_knn, y_train_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_knn_tuned_pred = KNN_tuned.predict(X_test_knn)\n",
    "\n",
    "mse_knn_tuned = mean_squared_error(y_test_knn,y_knn_tuned_pred)\n",
    "r2_knn_tuned = KNN_tuned.score(X_test_knn, y_test_knn)\n",
    "\n",
    "print(\"MSE (tuned KNN) =\", mse_knn_tuned)\n",
    "print(\"R2 score (tuned KNN) =\",r2_knn_tuned)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (KNN) = \",mse_knn)\n",
    "print(\"R2 score (KNN) =\",r2_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are the best KNN control parameters we found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNN_tuned.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. This value of `n_neighbors` is inconsistent with the peak in cross-validation score in the validation curve plot.\n",
    "\n",
    "WHY? Commenthere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalization Error\n",
    "\n",
    "Notice that all the above tuning happened while training on a single split (`X_train` and `y_train`).\n",
    "\n",
    "\n",
    "It's possible that that particular fold prefers a slightly different set of parameters than a different one - so to assess our generalization error, we need a further level of cross-validation.\n",
    "\n",
    "\n",
    "We can do this by passing a `GridSearchCV` model to the cross validation score calculator. This will take a few moments, as the grid search is carried out for each CV fold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "R2 = cross_val_score(KNN_tuned, X_scaled_knn, y, cv=shuffle_split, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meanR2,errR2 = np.mean(R2),np.std(R2)\n",
    "print(\"Mean score:\",meanR2,\"+/-\",errR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* Optimizing over control parameters (or hyper parameters) with grid search cross validation is a form of model selection.\n",
    "\n",
    "\n",
    "* When presented with new metadata samples, and asked to predict the target response variables (CPU time), we'll need a trained machine that has not been *over-fitted* to the training data.\n",
    "\n",
    "\n",
    "* Minimizing and estimating the generalization error is a way to reduce the risk of getting this prediction wrong. \n",
    "\n",
    "\n",
    "* Let's finish off our CPU time machine-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNNbest = KNN_tuned.best_estimator_\n",
    "# KNNbest.fit(X_train_knn, y_train_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will we actually use this tuned model? Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = 34\n",
    "one_pretend_run = X_test_knn[j,:]\n",
    "cpupredicted = KNNbest.predict(one_pretend_run)\n",
    "cpuactual = y_test_knn[j]\n",
    "print(\"True CPU cf. KNN predicted CPU time:\",cpuactual,cpupredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpuactual = y_test_knn\n",
    "cpupredicted = KNNbest.predict(X_test_knn)\n",
    "\n",
    "plot(cpuactual, cpupredicted,'o',alpha=0.2)\n",
    "title(\"KNNbest performance\")\n",
    "xlabel(\"CPU Time\")\n",
    "ylabel(\"Predicted CPU Time\")\n",
    "lims = [9.0,15]\n",
    "xlim(lims)\n",
    "ylim(lims)\n",
    "plot(lims, lims, ':k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression\n",
    "\n",
    "Now try the random forest regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RF = RandomForestRegressor()\n",
    "RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_RF = RF.predict(X_test)\n",
    "\n",
    "\n",
    "mse_RF = mean_squared_error(y_test,y_pred_RF)\n",
    "r2_RF = RF.score(X_test, y_test)\n",
    "\n",
    "print(\"MSE (RF) =\", mse_RF)\n",
    "print(\"R2 score (RF) =\",r2_RF)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (tuned KNN) =\", mse_knn_tuned)\n",
    "print(\"R2 score (tuned KNN) =\",r2_knn_tuned)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (KNN) =\", mse_knn)\n",
    "print(\"R2 score (KNN) =\",r2_knn)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (linear regression) = \",mse_linear)\n",
    "print(\"R2 score (linear regression) =\",r2_linear)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(y_test, y_pred_RF - y_test,'o',alpha=0.2)\n",
    "#plot(y_test, y_lr_pred - y_test,'x')\n",
    "title(\"RF Residuals - MSE = %.2f\" % mse_RF)\n",
    "xlabel(\"CPU Time\")\n",
    "ylabel(\"Residual\")\n",
    "ylim(-3,2)\n",
    "hlines(0,min(y_test),max(y_test),color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(y_test,y_lr_pred - y_test,'o',alpha=0.3, color= 'r', label= \"Linear MSE = %.2f\" % mse_linear)\n",
    "plot(y_test_knn, y_knn_pred - y_test_knn,'o',alpha=0.3, color= 'b', label= \"KNN MSE = %.2f\" % mse_knn)\n",
    "plot(y_test, y_pred_RF - y_test,'o',alpha=0.3, color= 'g', label= \"RF MSE = %.2f\" % mse_RF)\n",
    "title(\"Regression Residuals\")\n",
    "legend()\n",
    "xlabel(\"CPU Time\")\n",
    "ylabel(\"Residual\")\n",
    "ylim(-3,2)\n",
    "hlines(0,min(y_test),max(y_test),color=\"red\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run a grid search with Random Forests over n_estimators\n",
    "param_grid_RF = {'n_estimators': np.array([16,32,64, 128, 256, 512, 1024])}\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(param_grid_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_tuned = GridSearchCV(RF, param_grid_RF, verbose=3)\n",
    "\n",
    "RF_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_RF_tuned_pred = RF_tuned.predict(X_test)\n",
    "\n",
    "mse_RF_tuned = mean_squared_error(y_test,y_RF_tuned_pred)\n",
    "r2_RF_tuned = RF_tuned.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"MSE (tuned RF) =\", mse_RF_tuned)\n",
    "print(\"R2 score (tuned RF) =\",r2_RF_tuned)\n",
    "print(\"cf.\")\n",
    "\n",
    "print(\"MSE (RF) =\", mse_RF)\n",
    "print(\"R2 score (RF) =\",r2_RF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RFbest = RF_tuned.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpuactual_RF = y_test\n",
    "cpupredicted_RF = RFbest.predict(X_test)\n",
    "\n",
    "plot(cpuactual_RF, cpupredicted_RF,'o',alpha=0.2)\n",
    "title(\"RFbest performance\")\n",
    "xlabel(\"CPU Time\")\n",
    "ylabel(\"Predicted CPU Time\")\n",
    "lims = [9.0,15]\n",
    "xlim(lims)\n",
    "ylim(lims)\n",
    "plot(lims, lims, ':k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(cpuactual, cpupredicted,'o',alpha=0.2, label= 'KNNBest', color='b')\n",
    "plot(cpuactual_RF, cpupredicted_RF,'o',alpha=0.3, label= 'RFBest', color= 'g')\n",
    "\n",
    "title(\"Best performance\")\n",
    "xlabel(\"CPU Time\")\n",
    "ylabel(\"Predicted CPU Time\")\n",
    "lims = [9.0,15]\n",
    "xlim(lims)\n",
    "ylim(lims)\n",
    "plot(lims, lims, ':k')\n",
    "legend(loc= 0)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 7)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
