{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Phosim CPU Times: \n",
    "## What can we learn from Twinkles Run 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twinkles Run 1 is 1227 visits, a subset of the observations of a Deep Drilling Field from the kraken-1042 Opsim simulation.  Each of the visits is simulated as a 30-second observation with Phosim.  Together, the 1227 runs of Phosim required about 4 CPU years on the SLAC batch farm.  The CPU time required per visit varied widely.  Some required only a few hours.  More than 100 visits (representing about one third of the total CPU time) never completed because they reached the 5-day CPU time limit for the batch hosts, and produced no output.  \n",
    "\n",
    "Studying the dependence of the CPU time on the various inputs to the Phosim simulations has several motivations.  Certainly there's no sense in investing CPU time in a run that cannot finish with 5 CPU days.  And Run 1 was relatively modest in terms of resource requirements.  Essentially all 1227 simulations ran in parallel on the batch farm.  For Twinkles1 and 2, which will be much larger, the batch farm will not be able to run all of the simulations in parallel and the effective throughput could well be determined by the individual simulations that take the most CPU time, if they are allowed to hog the available hosts. Twinkles1 and 2 might run at NERSC, which has more hosts, but reportedly a 2-day CPU limit per host.  Absent some solution involving checkpointing, we may need to make judicious adjustments to the Phosim inputs for these runs.  Also, John Peterson has pointed out that we should not assume that all of the inputs to Phosim for Run 1 were sensible.\n",
    "\n",
    "For this study, a number of Phosim inputs were extracted from the instance catalogs for the runs, and combined with execution time information from the batch farm.  The columns of the resulting csv file are described [here](https://github.com/DarkEnergyScienceCollaboration/Twinkles/issues/159).\n",
    "\n",
    "This Notebook is based on Phil Marshall's [machine learning example](https://github.com/drphilmarshall/StatisticalMethods/blob/master/examples/SDSScatalog/Quasars.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For pretty plotting\n",
    "# !pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "%pylab inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import copy\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "* This is a regression problem, to be able to predict the CPU time in terms of various metadata related to the runs.  These apparently are called \"features\" in machine learning terminology.  Also the CPU time will be referred to as the \"response variable.\"\n",
    "\n",
    "* Read in the data.  Here just selected columns are being read from the file.  *They may or may not be the most relevant.  Also, the filter and hostname columns would need to be converted to numerical values in order to be used.  And the runlimit column probably needs special treatment because it is clearly related to large CPU times*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run1meta = pd.read_csv(\"http://www.slac.stanford.edu/~digel/lsst/run1_metadata.csv\",index_col=0,usecols=[\"obshistid\",\"expmjd\",\"filter\",\"rotskypos\",\"altitude\",\\\n",
    "                                                \"rawseeing\",\"airmass\",\"sunalt\",\"moonalt\",\"moonphase\",\"dist2moon\",\"cputime\",\\\n",
    "                                                \"runlimit\"])\n",
    "\n",
    "# Omit the two runs that have not actually finished running (or been \n",
    "# terminated at the CPU time limit).  These are flagged with -999.0 in\n",
    "# the input file.  Also omit the runs that reached the CPU limit:\n",
    "run1meta = run1meta[(run1meta[\"cputime\"] > 0.) & (run1meta[\"runlimit\"] == 0)]\n",
    "\n",
    "#run1meta = run1meta[(run1meta[\"cputime\"] > 0.)]\n",
    "\n",
    "\n",
    "# Response variables: redshift\n",
    "cputime = run1meta[\"cputime\"]\n",
    "\n",
    "# Features or attributes: photometric measurements\n",
    "run1meta_features = copy.copy(run1meta)\n",
    "del run1meta_features[\"cputime\"]\n",
    "del run1meta_features[\"runlimit\"]\n",
    "run1meta_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins =  hist(cputime.values,bins=100) ; xlabel(\"CPU time (s)\") ; ylabel(\"N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of CPU time has a very long tail, plus a number of runs piled up at the 5-day CPU time limit.\n",
    "\n",
    "Let's plot all the features, colored by the CPU time, to look for structure.  (Possibly the logarithm of the CPU time should be used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Truncate the color at 3e5 CPU sec just to keep some contrast.\n",
    "norm = mpl.colors.Normalize(vmin=min(cputime.values), vmax=300000)\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "# Plot everything against everything else:\n",
    "rez = pd.scatter_matrix(run1meta_features,alpha=0.2,figsize=[15,15],color=m.to_rgba(cputime.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our machine learning inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = run1meta_features.values  # Data: 5-d feature space\n",
    "y = cputime.values # Target: redshifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Design matrix shape =\", X.shape)\n",
    "print(\"Response variable vector shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Let's follow the same procedure as in the [`SciKit-Learn` tutorial](../../scikit-learn/Linear_Regression.ipynb) we just went through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "linear = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the model, using all the attributes:\n",
    "linear.fit(X_train, y_train)\n",
    "\n",
    "# Do the prediction on the test data:\n",
    "y_lr_pred = linear.predict(X_test)\n",
    "\n",
    "# How well did we do?\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_linear = np.sqrt(mean_squared_error(y_test,y_lr_pred))\n",
    "r2_linear = linear.score(X_test, y_test)\n",
    "print(\"Linear regression: MSE = \",mse_linear)\n",
    "print(\"R2 score =\",r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(y_test,y_lr_pred - y_test,'o',alpha=0.2)\n",
    "title(\"Linear Regression Residuals - MSE = %.2f\" % mse_linear)\n",
    "xlabel(\"CPU Time\")\n",
    "ylabel(\"Residual\")\n",
    "ylim(-300000,200000)\n",
    "hlines(0,min(y_test),max(y_test),color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just how bad is this? Here's the MSE from guessing the *average redshift of the training set* for all new objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Naive MSE\", ((1./len(y_train))*(y_train - y_train.mean())**2).sum())\n",
    "print(\"Linear regression: MSE = \",mse_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_squared_error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *k*-Nearest Neighbor (KNN) Regression\n",
    "\n",
    "Now let's try a different kind of model: a *non-parametric* one.\n",
    "\n",
    "[\"Regression based on k-nearest neighbors. The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.\"](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "\n",
    "#### Question:\n",
    "\n",
    "What underlying model is implied by the KNN algorithm? How many hidden parameters does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_scaled_knn = preprocessing.scale(X) # Many methods work better on scaled X.\n",
    "\n",
    "KNN = neighbors.KNeighborsRegressor(5)\n",
    "\n",
    "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X_scaled_knn, y)\n",
    "\n",
    "KNN.fit(X_train_knn,y_train_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_knn_pred = KNN.predict(X_test_knn)\n",
    "mse_knn = mean_squared_error(y_test_knn,y_knn_pred)\n",
    "r2_knn = KNN.score(X_test_knn, y_test_knn)\n",
    "print(\"MSE (KNN) =\", mse_knn)\n",
    "print(\"R2 score (KNN) =\",r2_knn)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (linear regression) = \",mse_linear)\n",
    "print(\"R2 score (linear regression) =\",r2_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(y_test_knn, y_knn_pred - y_test_knn,'o',alpha=0.2)\n",
    "#plot(y_test, y_lr_pred - y_test,'x')\n",
    "title(\"k-NN Residuals - MSE = %.2f\" % mse_knn)\n",
    "xlabel(\"CPU Time\")\n",
    "ylabel(\"Residual\")\n",
    "ylim(-300000,200000)\n",
    "hlines(0,min(y_test),max(y_test),color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tuning the KNN Model\n",
    "\n",
    "* Let's vary the control parameters of the KNN model, to see how good we can make our predictions.\n",
    "\n",
    "* We can see our options in the model `repr`:\n",
    "\n",
    "> KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_neighbors=5, p=2, weights='uniform')\n",
    "\n",
    "* Let's first make a \"validation curve\" to investigate one parameter: the number of nearest neighbors averaged over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll vary the number of neighbors used:\n",
    "param_name = \"n_neighbors\"\n",
    "param_range = np.array([1,2,4,8,16,32,64])\n",
    "\n",
    "# And we'll need a cv iterator:\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "shuffle_split = ShuffleSplit(len(X), 10, test_size=0.4)\n",
    "\n",
    "# Compute our cv scores for a range of the no. of neighbors:\n",
    "from sklearn.learning_curve import validation_curve\n",
    "training_scores, validation_scores = validation_curve(KNN, X_scaled, y,\n",
    "                                                      param_name=param_name,\n",
    "                                                      param_range=param_range, \n",
    "                                                      cv=shuffle_split, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_validation_curve(param_name,parameter_values, training_scores, validation_scores):\n",
    "    training_scores_mean = np.mean(training_scores, axis=1)\n",
    "    training_scores_std = np.std(training_scores, axis=1)\n",
    "    validation_scores_mean = np.mean(validation_scores, axis=1)\n",
    "    validation_scores_std = np.std(validation_scores, axis=1)\n",
    "\n",
    "    plt.fill_between(parameter_values, training_scores_mean - training_scores_std,\n",
    "                     training_scores_mean + training_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(parameter_values, validation_scores_mean - validation_scores_std,\n",
    "                     validation_scores_mean + validation_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(parameter_values, training_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(parameter_values, validation_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.ylim(validation_scores_mean.min() - .1, training_scores_mean.max() + .1)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_validation_curve(param_name, param_range, training_scores, validation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "Can you explain the shapes of these two curves? Talk to your neighbor for a few minutes, and be prepared to suggest reasons for a) the rise and fall of the cross validation score and b) the monotonic decrease in training score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model tuning with `GridSearchCV`\n",
    "\n",
    "* Now, let's see if we can do better by varying some other KNN options as well - in a *grid search*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': np.array([1,2,4,8,16,32,64]),\n",
    "                  'weights': ['uniform','distance'],\n",
    "                       'p' : np.array([1,2])}\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "KNN_tuned = GridSearchCV(KNN, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `GridSearchCV` object behaves just like a model, except it carries out a cross-validation while fitting:\n",
    "\n",
    "<img src=\"../../scikit-learn/figures/grid_search_cross_validation.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNN_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_knn_tuned_pred = KNN_tuned.predict(X_test)\n",
    "\n",
    "mse_knn_tuned = mean_squared_error(y_test,y_knn_tuned_pred)\n",
    "r2_knn_tuned = KNN_tuned.score(X_test, y_test)\n",
    "\n",
    "print(\"MSE (tuned KNN) =\", mse_knn_tuned)\n",
    "print(\"R2 score (tuned KNN) =\",r2_knn_tuned)\n",
    "print(\"cf.\")\n",
    "print(\"MSE (KNN) = \",mse_knn)\n",
    "print(\"R2 score (KNN) =\",r2_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are the best KNN control parameters we found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNN_tuned.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value of `n_neighbors` is consistent with the peak in cross-validation score in the validation curve plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalization Error\n",
    "\n",
    "Notice that all the above tuning happened while training on a single split (`X_train` and `y_train`).\n",
    "\n",
    "\n",
    "It's possible that that particular fold prefers a slightly different set of parameters than a different one - so to assess our generalization error, we need a further level of cross-validation.\n",
    "\n",
    "\n",
    "We can do this by passing a `GridSearchCV` model to the cross validation score calculator. This will take a few moments, as the grid search is carried out for each CV fold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "R2 = cross_val_score(KNN_tuned, X_scaled, y, cv=shuffle_split, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meanR2,errR2 = np.mean(R2),np.std(R2)\n",
    "print(\"Mean score:\",meanR2,\"+/-\",errR2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* Optimizing over control parameters (or hyper parameters) with grid search cross validation is a form of model selection.\n",
    "\n",
    "\n",
    "* When presented with new metadata samples, and asked to predict the target response variables (CPU time), we'll need a trained machine that has not been *over-fitted* to the training data.\n",
    "\n",
    "\n",
    "* Minimizing and estimating the generalization error is a way to reduce the risk of getting this prediction wrong. \n",
    "\n",
    "\n",
    "* Let's finish off our CPU time machine-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNNz = KNN_tuned.best_estimator_\n",
    "KNNz.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = 71\n",
    "one_pretend_run = X_test[j,:]\n",
    "cpupredicted = KNNz.predict(one_pretend_run)\n",
    "cpuactual = y_test[j]\n",
    "print(\"True CPU cf. KNN predicted CPU time:\",cpuactual,cpupredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpuactual = y_test\n",
    "cpupredicted = KNNz.predict(X_test)\n",
    "\n",
    "plot(cpuactual, cpupredicted,'o',alpha=0.1)\n",
    "title(\"KNNz performance\")\n",
    "xlabel(\"CPU Time\")\n",
    "ylabel(\"Predicted CPU Time\")\n",
    "lims = [0.0,450e3]\n",
    "xlim(lims)\n",
    "ylim(lims)\n",
    "plot(lims, lims, ':k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasar Classification with Random Forests\n",
    "\n",
    "\n",
    "* Let's switch gears and do a 3-class classification problem: star, galaxy, or QSO.\n",
    "\n",
    "\n",
    "* A very good general-purpose classification (and regression!) algorithm is Random Forest. See [this blog post](http://blog.yhathq.com/posts/random-forests-in-python.html) for a nice high level introduction.\n",
    "\n",
    "\n",
    "* [\"A random forest is a meta estimator that fits a number of *decision tree classifiers* on various sub-samples of the dataset, and uses averaging to improve the predictive accuracy and control over-fitting.](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "\n",
    "* Let's read in equal numbers of all three types of data, clean them up, and set $y$ equal to the classification label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sources = pd.read_csv(\"data/qso10000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"])[:1000]\n",
    "\n",
    "all_sources = all_sources.append(pd.read_csv(\"data/star1000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"]))\n",
    "\n",
    "all_sources = all_sources.append(pd.read_csv(\"data/galaxy1000.csv\",index_col=0,usecols=[\"objid\",\"dered_r\",\"u_g_color\",\\\n",
    "                                                \"g_r_color\",\"r_i_color\",\"i_z_color\",\"diff_u\",\\\n",
    "                                                \"diff_g1\",\"diff_i\",\"diff_z\",\"class\"]))\n",
    "\n",
    "all_sources = all_sources[(all_sources[\"dered_r\"] > -9999) & (all_sources[\"g_r_color\"] > -10) & (all_sources[\"g_r_color\"] < 10)]\n",
    "\n",
    "all_labels = all_sources[\"class\"]\n",
    "\n",
    "all_features = copy.copy(all_sources)\n",
    "del all_features[\"class\"]\n",
    "\n",
    "X = copy.copy(all_features.values)\n",
    "y = copy.copy(all_labels.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_labels.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Feature vector shape =\", X.shape)\n",
    "print(\"Class label vector shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What structure can we see in the data? Let's plot all the features as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yy = all_labels.values.copy()\n",
    "yy[yy==\"QSO\"] = 0.0    # Red\n",
    "yy[yy==\"STAR\"] = 0.5   # Green\n",
    "yy[yy==\"GALAXY\"] = 1.0 # Blue\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=min(yy), vmax=max(yy))\n",
    "cmap = cm.jet_r\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "rez = pd.scatter_matrix(all_features,alpha=0.2,figsize=[15,15],color=m.to_rgba(yy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - looks like there is information there to be used! \n",
    "Let's turn on the machine learning.\n",
    "\n",
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100,oob_score=True)\n",
    "rf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the important features in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(zip(all_sources.columns.values,rf.feature_importances_),key=lambda q: q[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the \"Out of Bag\" accuracy (of predicted y compared to truth), made available by ensemble classifiers. (Each decision tree in the ensemble is only working on a subset of the data, so it can track its accuracy with the data not in its own bag.) \n",
    "\n",
    "The accuracy of a classifier is the fraction of predictions made that are correct. This one looks like its doing well - but this is the accuracy on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier improvement with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameter values to try:\n",
    "parameters = {'n_estimators':(50,100,200),\"max_features\": [\"auto\",3],\n",
    "              'criterion':[\"gini\",\"entropy\"],\"min_samples_leaf\": [1,2]}\n",
    "\n",
    "# Initial training/test split:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do a grid search to find the highest 3-fold CV score:\n",
    "rf_tuned = GridSearchCV(rf, parameters, cv=3, verbose=1)\n",
    "RFselector = rf_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Print the best score and estimator:\n",
    "print(RFselector.best_score_)\n",
    "print(RFselector.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "Would you be satisfied with a 95% successful classification fraction? Read the Random Forest `SciKit-Learn` docs to find some alternative scores, and think about when you might want to choose one of these instead. (Hint: imagine using a classifier to select a sample of *targets*.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of visualizing classification accuracy is via a *confusion matrix*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = RFselector.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output label comes with a *classification probability*, computed from the results of the whole forest. To select a sample of classified objects, one can choose a selection threshold in this class probability, and only keep objects with higher probability than this threshold.\n",
    "\n",
    "\n",
    "The availability of a class probability leads to an important diagnostic: the \"Receiver Operating Characteristic\" or [\"ROC\" curve](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html). This shows the *true positive rate* (TPR) plotted against the *false positive rate* (FPR) of a classifier, as the selection threshold is varied.\n",
    "\n",
    "\n",
    "Typically, classifiers have control parameters that affect both the TPR and FPR (often improving one at the expense of the other), so the ROC curve is a good tool for investigating these parameters. \n",
    "\n",
    "\n",
    "Likewise, ROC curves provide a very good way to compare different classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Use `SciKit-Learn` utilities to plot an ROC curve for the RFselector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Back to the lesson plan](../../lessons/9.MachineLearning.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
