# Lessons Learned

## Run 1 and 1.1:

* **Getting all the astrometry right is hard.**  There are differential effects we want to simulate with catsim so things move around correctly.  Unfortunately, we found that bulk precession ended up in the WCS produced by phosim which made our reference catalogs useless. (Simon)
* **Enabling data access is important.** It took me a while to figure out how to get to the data in a place where I could look at it.  This was mostly just figuring out which machine to use and paths to the data and stack.  Maybe Tony's concept of a portal will help this. (Simon)
* **Epics are really useful** for managing large numbers of GitHub issues. We now have most of our issues organized into epics, and a labeling system that allows 1) only the epics themselves to be shown in a high-level view, and 2) all issues in a particular epic to be focused on - both by using issue filtering. (Phil)
* **Conda installation of the DM Stack can be carried out by Travis CI** allowing us to use this service. (Phil)
* **Using both the LSST sims tools and the DM stack in the same project was surprisingly difficult.** Hopefully this will improve as a result of our trying to do it! (Phil)
* **We are already close to the edge of what the DM Stack can do,** and as a result, discussing how to emulate the forced photometry of DIAObjects, for example. Are we about to become Stack developers? (Phil)
* **The CPU time required to simulate a 30-s visit in Phosim ranges over two orders of magnitude,** depending mostly on the brightness of the sky, which depends mostly on whether the moon is up.  The overall time required is dominated by these visits, and in fact 103 of the 1227 Run 1 simulations reached the 120-hour batch farm limit at SLAC, and so did not finish.  This has implications for scheduling and running simulations at NERSC, where the CPU time limits per job are less, and suggests that checkpointing may be needed in the workflow. (Seth)
* **Simulation times**  We have the capability of simulating a catsim instance catalog, phosim instance catalog and phosim images. The times taken to do these are phosim instance catalog < catsim instance catalog << phosim images. While the phosim instance catalogs are the required input for phosim images, the catsim instance catalogs have a lot more information and should be made at the same time for analysis. The time required for making both of these is negligible compared to the time required for making the phosim images, so doing all of them together is probably the best way. While this was hard due to access issues, this is no longer the case with appropriate access made available :). Moreover, this will get rid of an onerous step of moving data from one machine to another. We have a simple validation script that checks that the expected csv file exists and has over a minimal size, but we should try to improve this. (Rahul)
* **Instance Catalogs**  Currently, there is no way to obtain an instance catalog as a structured array/astropy Table or a pandas DataFrame, they can only be written to disk in the form of a csv file, which must be read in. There is a single phosim and catsims instance catalog (and file) for each pointing leading to many files. Among other things, this large number of files uses up inodes, and the problem (as well as performance) is likely to get better by using hdf5 files. A good design in hdf files should be chosen. (Rahul) [SD: Each of the pointings had ~5000 files (200 Mbyte uncompressed) of instance catalog information, mostly SEDs.  This was in addition to the sims_sed_library (~1 Gbyte) that was copied into a scratch space for each phosim run.]
* **pandas, precision and Instance Catalogs**  Since the integer ids in instance catalogs get very large (this is to ensure unique indices across all classes of objects), a direct `pandas.read_csv` does not work eith the largest integers due to precisions issues. This can be circumvented by using `pandas.read_csv(..., float_precision='high')` (Rahul)
* **design and precision** Phosim reads in these values as floats, and can be off by a few as we have found in the case of lenses. There is no quick remedy to this like above. (Rahul)
* **A small dataset** is important for being able to run exammples and tests (Rahul)
* **Position angles can be deceptive** I interpreted the orientation of galaxis incorrectly, so there are many SN outside the disk of the galaxy.
* **NUmbers of SN** I think we created a run with too many SN, and we should  cull the numbers down next time.

## Run 2:

* **At NERSC, it matters a lot for throughput where the software is installed.** (Tony)
* **We need to stay on top of the DMstack (apps and sims) development** both in terms of determining the versions to install but also to sort out problems as they are encountered. This is best done either through [community.lsst.org](http://community.lsst.org) or via GitHub issues (Heather)
* **Conda installs of DMstack are great, but may not aways be available** In that case, brush up on your use of [lsstsw](https://developer.lsst.io/build-ci/lsstsw.html). (Heather)
* **PhoSim Centroid files list sourceID incorrectly for large integers** It seems that id numbers in PhoSim are stored as double precision floats and id numbers from Instance Catalogs go long enough that they are outside this range of about 2^53 ~ 9 * 10^15 and get rounded off. Therefore, they don't get stored in PhoSim Centroid files correctly. We solved this in Twinkles by making all our id numbers smaller by 10^8 because they can still be unique in our smaller field. However, due to fact that this hack will not be available to larger fields this is still an [open issue in phosim](https://jira.lsstcorp.org/browse/PHOSIM-27).
